# -*- coding: utf-8 -*-
"""training_v1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1k6PIxk3JRJDhKyknLa5MITVPl0WYOVbZ
"""

!pip install darts

import pandas as pd
from darts import TimeSeries
from darts.dataprocessing.transformers import Scaler
from darts.models import TCNModel
from darts import TimeSeries
from darts.ad.utils import (
    eval_metric_from_binary_prediction,
    eval_metric_from_scores,
    show_anomalies_from_scores,
)
from darts.ad import (
    ForecastingAnomalyModel,
    KMeansScorer,
    NormScorer,
    WassersteinScorer,
)

# Load the data (replace 'train.txt' and 'test.txt' with your actual file names)
train_data = pd.read_csv('ECG5000_TRAIN.txt', delim_whitespace=True, header=None)
test_data = pd.read_csv('ECG5000_TEST.txt', delim_whitespace=True, header=None)

# Check for null values in both datasets
print("Null values in training data:", train_data.isnull().sum().sum())
print("Null values in testing data:", test_data.isnull().sum().sum())

# Merge the datasets row-wise
combined_data = pd.concat([train_data, test_data], axis=0).reset_index(drop=True)

train_final = combined_data[combined_data[0] == 1].reset_index(drop=True)
test_final = combined_data[combined_data[0] != 1].reset_index(drop=True)

# Drop the label column (0th column)
train_final = train_final.drop(columns=[0])
test_final = test_final.drop(columns=[0])

# Convert to TimeSeries objects
series = TimeSeries.from_dataframe(train_final)
test_series = TimeSeries.from_dataframe(test_final)

# Manually split the data into train and validation sets (e.g., 80% train, 20% val)
train_size = int(0.8 * len(series))
train_series = series[:train_size]
val_series = series[train_size:]

# Normalize the data using Darts Scaler
scaler = Scaler()
train_series_scaled = scaler.fit_transform(train_series)
val_series_scaled = scaler.transform(val_series)
test_series_scaled = scaler.transform(test_series)

from pytorch_lightning.callbacks.early_stopping import EarlyStopping

# stop training when validation loss does not decrease more than 0.05 (`min_delta`) over
# a period of 5 epochs (`patience`)
my_stopper = EarlyStopping(
    monitor="val_loss",
    patience=5,
    min_delta=0.05,
    mode='min',
)

# Define and train the TCN model without covariates
model = TCNModel(
    input_chunk_length=30,  # Adjust based on your data
    output_chunk_length=10,  # Adjust based on desired forecast horizon
    n_epochs=100,            # Number of epochs for training
    dropout=0.1,             # Dropout rate to prevent overfitting
    random_state=42 ,
    pl_trainer_kwargs={"callbacks": [my_stopper]}
)

# Fit the model on the training data
model.fit(series=train_series_scaled, val_series=val_series_scaled, epochs = 20)

import torch

# Assuming `model` is your trained model
# torch.save(model.state_dict(), 'model.pth')
torch.save(model, 'full_model.pth')

import matplotlib.pyplot as plt
import numpy as np

# Number of samples to visualize
num_samples = 5

# Prepare the plot
plt.figure(figsize=(15, num_samples * 5))

for i in range(num_samples):
    # Extract the i-th validation series
    val_series_sample = val_series_scaled[i]

    # Predict using the model
    prediction = model.predict(n=len(val_series_sample))

    # Convert the TimeSeries objects to numpy arrays for plotting
    actual_values = val_series_sample.pd_dataframe().values.flatten()
    predicted_values = prediction.pd_dataframe().values.flatten()

    # Plot the results
    plt.subplot(num_samples, 1, i + 1)
    plt.plot(actual_values, label='Actual Values', color='blue')
    plt.plot(predicted_values, label='Predicted Values', color='red', linestyle='--')
    plt.title(f'Sample {i + 1}')
    plt.xlabel('Time')
    plt.ylabel('Value')
    plt.legend()
    plt.grid(True)

# Adjust layout
plt.tight_layout()
plt.show()

import matplotlib.pyplot as plt

# Number of test samples to visualize
num_test_samples = min(5, len(test_series_scaled))  # Adjust to the number of available test samples

# Prepare the plot
plt.figure(figsize=(15, num_test_samples * 5))

for i in range(num_test_samples):
    # Extract the i-th test series
    test_series_sample = test_series_scaled[i]

    # Predict using the model
    prediction = model.predict(n=len(test_series_sample))

    # Convert the TimeSeries objects to numpy arrays for plotting
    actual_values = test_series_sample.pd_dataframe().values.flatten()
    predicted_values = prediction.pd_dataframe().values.flatten()

    # Plot the results
    plt.subplot(num_test_samples, 1, i + 1)
    plt.plot(actual_values, label='Actual Values', color='blue')
    plt.plot(predicted_values, label='Predicted Values', color='red', linestyle='--')
    plt.title(f'Test Sample {i + 1}')
    plt.xlabel('Time')
    plt.ylabel('Value')
    plt.legend()
    plt.grid(True)

# Adjust layout
plt.tight_layout()
plt.show()

import numpy as np
import matplotlib.pyplot as plt

# Parameters
chunk_size = 10
threshold = 0.5  # Set an appropriate threshold based on your data and requirements

# Initialize lists for plotting
errors = []
colors = []

# Loop through test series in chunks
for start_idx in range(0, len(test_series_scaled) - chunk_size, chunk_size):
    # Extract the current chunk
    current_chunk = test_series_scaled[start_idx:start_idx + chunk_size]

    # Predict the next chunk
    prediction = model.predict(n=chunk_size)

    # Convert the TimeSeries objects to numpy arrays
    actual_values = current_chunk.pd_dataframe().values.flatten()
    predicted_values = prediction.pd_dataframe().values.flatten()

    # Calculate the error between actual and predicted values
    error = np.mean(np.abs(actual_values - predicted_values))

    # Determine if the chunk is anomalous
    is_anomalous = error > threshold

    # Store error and color for plotting
    errors.append(error)
    colors.append('red' if is_anomalous else 'green')

# Prepare the plot
plt.figure(figsize=(12, 6))

# Plot errors with color coding
for i in range(len(errors)):
    plt.axvline(x=i * chunk_size, color=colors[i], linestyle='--', linewidth=1.5)

plt.plot(np.arange(len(errors)) * chunk_size, errors, marker='o', linestyle='-', color='black', label='Error')
plt.xlabel('Time')
plt.ylabel('Error')
plt.title('Window-wise Anomaly Detection')
plt.legend()
plt.grid(True)

# Show the plot
plt.tight_layout()
plt.show()

anomaly_model = ForecastingAnomalyModel(
    model=model,
    scorer=[
        NormScorer(ord=1),
        # WassersteinScorer(window=30, window_agg=False),
        # WassersteinScorer(window=30, window_agg=True),
    ],
)

START = 0.1
anomaly_model.fit(train_series, start=START, allow_model_training=False, verbose=True)

anomaly_scores, model_forecasting = anomaly_model.score(
    test_series, start=0.1, return_model_prediction=True, verbose=True
)
pred_start = model_forecasting.start_time()

#  compute the MAE and RMSE on the test set
print(
    "On testing set -> MAE: {}, RMSE: {}".format(
        mae(model_forecasting, test_series), rmse(model_forecasting, test_series)
    )
)

# # plot the data and the anomalies
# fig, ax = plt.subplots(figsize=(15, 5))
# test_series.plot(label="Number of taxi passengers")
# model_forecasting.plot(label="Prediction of the model", linewidth=0.9)
# plt.show()

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from darts import TimeSeries
from darts.models import TCNModel
from darts.ad import ForecastingAnomalyModel
from darts.ad.scorers import NormScorer, WassersteinScorer
from darts.metrics import mae, rmse

# Suppress logs
import logging
logging.getLogger("pytorch_lightning.utilities.rank_zero").addHandler(logging.NullHandler())
logging.getLogger("pytorch_lightning.accelerators.cuda").addHandler(logging.NullHandler())

# Assuming `model` is already fitted TCNModel and `train_series` and `test_series` are prepared
anomaly_model = ForecastingAnomalyModel(
    model=model,
    scorer=[
        NormScorer(ord=1),
        WassersteinScorer(window=30, window_agg=False),
        WassersteinScorer(window=30, window_agg=True),
    ],
)

# Fit the anomaly model on the training series
START = 0.1
anomaly_model.fit(train_series, start=START, allow_model_training=False, verbose=True)

# Chunk size
chunk_size = 10  # Change as needed

# Initialize lists to store results
anomaly_scores_list = []
chunk_indices = []

# Function to compute and plot anomaly scores
def plot_anomaly_scores(scores, chunks, threshold):
    plt.figure(figsize=(14, 7))
    plt.plot(chunks, scores, label='Anomaly Scores', color='blue', marker='o')
    plt.axhline(y=threshold, color='red', linestyle='--', label='Anomaly Threshold')
    plt.xlabel('Chunk Index')
    plt.ylabel('Anomaly Score')
    plt.title('Anomaly Scores vs Chunk Index')
    plt.legend()
    plt.show()

# Iterate over chunks in the test series
for i in range(0, len(test_series) - chunk_size + 1, chunk_size):
    chunk = test_series[i:i + chunk_size]
    anomaly_scores, _ = anomaly_model.score(chunk, start=START, return_model_prediction=False, verbose=False)

    # Assume the first score in the list is representative of the entire chunk
    avg_score = np.mean(anomaly_scores)
    anomaly_scores_list.append(avg_score)
    chunk_indices.append(i // chunk_size)

# Define a threshold for anomaly detection
threshold = np.percentile(anomaly_scores_list, 95)  # Example threshold, can be tuned

# Plot the results
plot_anomaly_scores(anomaly_scores_list, chunk_indices, threshold)

# Determine anomalies based on the threshold
anomalies = np.array(anomaly_scores_list) > threshold
print(f"Detected anomalies at chunk indices: {np.where(anomalies)[0]}")