# -*- coding: utf-8 -*-
"""TimeSeries_AnomalyDetection_TCNN.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1UjwDmR0U_EKKr7BdM44vcECBWOmGWt8A
"""

!pip install darts
!pip install dask[dataframe]

import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from darts import TimeSeries
from darts.models import TCNModel
from darts.metrics import mape, mae
import torch
import torch.nn as nn
import torch.optim as optim
import matplotlib.pyplot as plt

df_train = pd.read_csv('ECG5000_TRAIN.txt', delim_whitespace=True, header=None)
print(f'The original train data shape is: {df_train.shape}')
print(df_train.head())

# loading the test data using pandas
df_test = pd.read_csv('ECG5000_TEST.txt', delim_whitespace=True, header=None)
print(f'The original test data shape is: {df_test.shape}')
print(df_test.head())

# merging the both datasets
df = pd.concat([df_train, df_test], axis=0, ignore_index=True)
print(f'The merged data shape now is: {df.shape}')
print(df.head())

# Checking for null values
null_values = df.isnull().sum()
print(f'Null values in each column:\n{null_values}')

# Checking for duplicate rows
duplicate_rows = df.duplicated().sum()
print(f'Number of duplicate rows: {duplicate_rows}')

# Print the rows that are duplicated (if any)
if duplicate_rows > 0:
    print(f'Duplicate rows:\n{df[df.duplicated()]}')

# Basic statistics of the dataset
print(f'Statistics of given data: {df.describe()}')

# Checking the distribution of the target variable (assuming the first column is the target)
target_distribution = df.iloc[:, 0].value_counts()
print(f'Distribution of target variable:\n{target_distribution}')

# Dividing into normal and abnormal
# firstly, normal data with values == 1
df_normal = df[df[0] == 1]
print(f'The shape of normal data is: {df_normal.shape}')
# print(df_normal.head())

# abnormal data with values == 2
df_abnormal2 = df[df[0] == 2]
print(f'The shape of abnormal_2 data is: {df_abnormal2.shape}')

# abnormal data with values == 3
df_abnormal3 = df[df[0] == 3]
print(f'The shape of abnormal_3 data is: {df_abnormal3.shape}')

# abnormal data with values == 4
df_abnormal4 = df[df[0] == 4]
print(f'The shape of abnormal_4 data is: {df_abnormal4.shape}')

# abnormal data with values == 5
df_abnormal5 = df[df[0] == 5]
print(f'The shape of abnormal_5 data is: {df_abnormal5.shape}')

def plot_sample(df_input, ax, title):
    data = df_input
    first_row = data.iloc[0, 1:]

    ax.plot(first_row.index, first_row.values, marker='o')
    ax.set_title(title)
    ax.set_xlabel('Sample Time')
    ax.set_ylabel('Values Obtained')
    ax.grid(True)


def plot_all_samples(df_1, df_2, df_3, df_4, df_5):
    fig, axs = plt.subplots(3, 2, figsize=(10, 12))

    plot_sample(df_1, axs[0, 0], 'First Row of Normal Sample')
    plot_sample(df_2, axs[1, 0], 'First Row of Abnormal Sample 2')
    plot_sample(df_3, axs[2, 0], 'First Row of Abnormal Sample 3')
    plot_sample(df_4, axs[0, 1], 'First Row of Abnormal Sample 4')
    plot_sample(df_5, axs[1, 1], 'First Row of Abnormal Sample 5')

    # Hide the empty subplot
    fig.delaxes(axs[2, 1])

    # Adjust layout
    plt.tight_layout()

    # Add spacing between subplots
    plt.subplots_adjust(hspace=0.5, wspace=0.3)

    plt.show()
plot_all_samples(df_normal, df_abnormal2, df_abnormal3, df_abnormal4, df_abnormal5)

# concat all abnormal data
df_abnormal = pd.concat([df_abnormal2, df_abnormal3, df_abnormal4, df_abnormal5], ignore_index=True)
print(f'abnormal data shape is: {df_abnormal.shape}')

scaler = StandardScaler()

y_train = df_normal[0]
X_train = df_normal.drop(columns=[0], axis=1)

y_test = df_abnormal[0]
X_test = df_abnormal.drop(columns=[0], axis=1)

# Fit the scaler on the training data and transform both training and test data
X_train_normalized = scaler.fit_transform(X_train)
X_test_normalized = scaler.transform(X_test)

# Convert back to DataFrame to print first few rows
X_train_normalized_df = pd.DataFrame(X_train_normalized, columns=X_train.columns)
X_test_normalized_df = pd.DataFrame(X_test_normalized, columns=X_test.columns)

# print(X_train.head())
# print(X_train_normalized_df.head())


X_train_split, X_val_split, y_train_split, y_val_split = train_test_split(X_train_normalized, y_train, test_size=0.2,
                                                                          random_state=42)
print(f'X_train_split shape is : {X_train_split.shape} \n y_train_split shape is : {y_train_split.shape} \n '
      f'X_val_split shape is : {X_val_split.shape} \n y_val_split shape is : {y_val_split.shape} ')

# Save the preprocessed datasets as CSV files
pd.DataFrame(X_train_split).to_csv('X_train_split.csv', index=False)
pd.DataFrame(X_val_split).to_csv('X_val_split.csv', index=False)
pd.DataFrame(y_train_split).to_csv('y_train_split.csv', index=False)
pd.DataFrame(y_val_split).to_csv('y_val_split.csv', index=False)

pd.DataFrame(X_test_normalized).to_csv('X_test_normalized.csv', index=False)
pd.DataFrame(y_test).to_csv('y_test.csv', index=False)

# Ensure GPU is available
device = 'cuda' if torch.cuda.is_available() else 'cpu'
print(f'Using device: {device}')

# Assuming the data is in suitable format and loaded into TimeSeries objects
train_series = TimeSeries.from_dataframe(pd.read_csv('X_train_split.csv'))
val_series = TimeSeries.from_dataframe(pd.read_csv('X_val_split.csv'))

import matplotlib.pyplot as plt
from darts.models import TCNModel
from pytorch_lightning.callbacks.early_stopping import EarlyStopping

# Assuming train_series and val_series are your TimeSeries objects for training and validation data

# Define the TCN model
model = TCNModel(input_chunk_length=30,
                 output_chunk_length=1,
                 kernel_size=2,
                 num_filters=3,
                 dilation_base=2,
                 dropout=0.1,
                 random_state=42)

# Define the EarlyStopping callback
early_stopping = EarlyStopping(
    monitor='val_loss',
    patience=5,
    min_delta=0.001,
    mode='min',  # 'min' because we're monitoring loss, and lower is better
    verbose=True
)

# Fit the model with early stopping by passing the callback through pl_trainer_kwargs
model.fit(series=train_series,
          val_series=val_series,
          epochs=100,  # Set to a higher number to allow early stopping to kick in
          verbose=True,
          pl_trainer_kwargs={"callbacks": [early_stopping]})

# Since the fit method does not return history, extract the training/validation losses from the model's trainer
history = model.trainer.callback_metrics

# Extract and plot the training/validation losses
train_losses = history['train_loss']
val_losses = history['val_loss']

# Plotting the training and validation losses
plt.plot(train_losses, label='Training Loss')
plt.plot(val_losses, label='Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.show()

print(history)

# Predict on the validation set
pred_series = model.predict(n=len(val_series), series=train_series)

# Evaluate the model
print("Mean Absolute Percentage Error (MAPE):", mape(val_series, pred_series))
print("Mean Absolute Error (MAE):", mae(val_series, pred_series))

# Load the test dataset
X_test_normalized_df = pd.read_csv('X_test_normalized.csv')
y_test_df = pd.read_csv('y_test.csv')

# Create TimeSeries object for test dataset
test_series = TimeSeries.from_dataframe(X_test_normalized_df)

# Make predictions on the test dataset
predictions = model.predict(n=len(test_series))

# Convert predictions to a DataFrame
predictions_df = predictions.pd_dataframe()

# Save predictions to a CSV file
predictions_df.to_csv('predictions.csv', index=False)